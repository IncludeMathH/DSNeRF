{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from thop import profile\n",
    "import torch\n",
    "from run_nerf_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'CA_Vim'\n",
    "\n",
    "netdepth = 8\n",
    "netwidth = 256\n",
    "output_ch = 5\n",
    "use_viewdirs = True\n",
    "device = 'cuda'\n",
    "skips = [4]\n",
    "\n",
    "input_ch = 63\n",
    "input_ch_views = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! You are using Mamba **Cross Attention + ViM** model!\n"
     ]
    }
   ],
   "source": [
    "if model_type == 'v1':\n",
    "    model = NeRF_mamba(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "# TODO: add model type v2\n",
    "elif model_type == 'v3':\n",
    "    model = NeRF_CA_mamba(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'Vim':\n",
    "    model = NeRF_Vim(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'VimCm':\n",
    "    model = NeRF_VimCm(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'CA_Vim':\n",
    "    model = NeRF_CA_Vim(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "else:\n",
    "    print(f'you are using original NeRF now')\n",
    "    model = NeRF(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! You are using Mamba **Cross Attention + ViM** model!\n"
     ]
    }
   ],
   "source": [
    "netdepth_fine = 8\n",
    "netwidth_fine = 256\n",
    "\n",
    "if model_type == 'v1':\n",
    "    model_fine = NeRF_mamba(D=netdepth_fine, W=netwidth_fine,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'v3':\n",
    "    model_fine = NeRF_CA_mamba(D=netdepth_fine, W=netwidth_fine,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'Vim':\n",
    "    model_fine = NeRF_Vim(D=netdepth_fine, W=netwidth_fine,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'VimCm':\n",
    "    model_fine = NeRF_VimCm(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "elif model_type == 'CA_Vim':\n",
    "    model_fine = NeRF_CA_Vim(D=netdepth, W=netwidth,\n",
    "                input_ch=input_ch, output_ch=output_ch,\n",
    "                input_ch_views=input_ch_views, use_viewdirs=use_viewdirs,\n",
    "                device=device,\n",
    "                ).to(device)\n",
    "else:\n",
    "    print(f'you are using original NeRF now')\n",
    "    model_fine = NeRF(D=netdepth_fine, W=netwidth_fine,\n",
    "                    input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                    input_ch_views=input_ch_views, use_viewdirs=use_viewdirs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m input_fine_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m90\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 获取参数数量\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 获取FLOPs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39minput_size)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data1/dn/project/DSNeRF/run_nerf_helpers.py:1418\u001b[0m, in \u001b[0;36mNeRF_CA_Vim.forward\u001b[0;34m(self, x, inference_params)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;66;03m# Cross Attention\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m q, kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpts_embed(input_pts), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mview_embed(input_views)\n\u001b[0;32m-> 1418\u001b[0m feature_fused \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1419\u001b[0m feature_fused \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(feature_fused) \u001b[38;5;241m+\u001b[39m q\n\u001b[1;32m   1420\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(feature_fused)\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torchsummary/torchsummary.py:22\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     20\u001b[0m summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m---> 22\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(o\u001b[38;5;241m.\u001b[39msize())[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.conda/envs/vim/lib/python3.10/site-packages/torchsummary/torchsummary.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     22\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 23\u001b[0m         [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "input_size = (64, 90)  # 例如，对于224x224的RGB图像\n",
    "input_fine_size = (128, 90)\n",
    "\n",
    "# 获取参数数量\n",
    "summary(model, input_size)\n",
    "\n",
    "# 获取FLOPs\n",
    "input = torch.randn(*input_size).cuda()\n",
    "# macs, params = profile(model, inputs=(input, ))\n",
    "# print('FLOPs: %.2f' % (macs * 2))  # 乘以2是因为乘法和加法都算作一次FLOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 128, 256]          16,384\n",
      "         LayerNorm-2             [-1, 128, 256]             512\n",
      "             Mamba-3             [-1, 128, 256]               0\n",
      "             Block-4  [[-1, 128, 256], [-1, 128, 256]]               0\n",
      "         LayerNorm-5             [-1, 128, 256]             512\n",
      "            Linear-6               [-1, 128, 1]             257\n",
      "         LayerNorm-7             [-1, 128, 283]             566\n",
      "             Mamba-8             [-1, 128, 283]               0\n",
      "             Block-9  [[-1, 128, 283], [-1, 128, 283]]               0\n",
      "        LayerNorm-10             [-1, 128, 283]             566\n",
      "           Linear-11               [-1, 128, 3]             852\n",
      "================================================================\n",
      "Total params: 19,649\n",
      "Trainable params: 19,649\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 18201.29\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 18201.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 获取参数数量\n",
    "summary(model_fine, input_fine_size)\n",
    "\n",
    "# 获取FLOPs\n",
    "input = torch.randn(*input_fine_size).cuda()\n",
    "# macs, params = profile(model_fine, inputs=(input, ))\n",
    "# print('FLOPs: %.2f' % (macs * 2))  # 乘以2是因为乘法和加法都算作一次FLOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
